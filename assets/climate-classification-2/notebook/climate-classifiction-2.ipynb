{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is the second part of the discussion of classifying climates. In the previous [one](https://bhornung.github.io/python/2019/01/29/climate-classification.html) we explored the structure of the data and optimised a k nearest neighbour classifier. The objectives this time are\n",
    "\n",
    "* build a data pipeline\n",
    "* use neural networks to classify the climate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "The data comprises of 24 numerical records and a label. The first twelve dimensions of the numerals are the monthly mean temperature in celsius degrees times ten. The second half lists the monthly mean precipitation in milimetres. \n",
    "\n",
    "### Data flow\n",
    "\n",
    "Much like in a real world, scenario we do not posses the entire dataset in advance. Each step in the data pipeline therefore has to be reuseable. In the previous post the data was preprocessed before the feature engineering step. This time, the preprocessing steps are incorporated in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Transformation\n",
    "\n",
    "The temperature and precipitation data are in different ranges and heavily riddled with outliers. We therefore use Yeo--Johnson transforms separately on the two sets.  \n",
    "\n",
    "#### Power transform on flattened arrays\n",
    "\n",
    "All features in a set should undergo identical transformation to preserve the relative positions of the data e.g. July's temperatures should be transformed exactly in the same way as December's temperatures are. For the transformer classes operate along each dimension, we write a wrapper which takes a 2D array and flattens it to **1D** before the fitting and transformation. It restores the original shape of the transformed array afterwards. The wrapper is implemented in the `create_flat_transformer` function below. It takes a transformer class and decorates its `fit`, `transform` and `fit_transform` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flat_transformer(transformer):\n",
    "    \"\"\"\n",
    "    Wrapper to perform the transformation on the flattened input array to 1D\n",
    "    The X argument is transformmed:\n",
    "        (n_samples, n_features) --> (n_samples * n_features, 1)\n",
    "        \n",
    "    It preserves the methods and attributes of the underlying estimator.\n",
    "    Parameters:\n",
    "        transformer (object) : should implement the methods\n",
    "            fit(X[, y])\n",
    "            transform(X)\n",
    "            \n",
    "    Returns\n",
    "        inner (object) : class with decorated methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    class inner(transformer):\n",
    "        def __init__(self, **kwargs):\n",
    "            \"\"\"\n",
    "            Instantiates a transformer object.\n",
    "            \"\"\"\n",
    "             \n",
    "            # sanity check \n",
    "            required_attrs = ['fit', 'transform', 'fit_transform']\n",
    "            for attr in required_attrs:\n",
    "                if getattr(super(), attr, None) is None:\n",
    "                    raise AttributeError(\"transformer should have {0} attribute\".format(attr))\n",
    "             \n",
    "            # use underlying estimators machinery\n",
    "            super().__init__( **kwargs)\n",
    "\n",
    "        def fit(self, X, y = None):\n",
    "            \"\"\"\n",
    "            1. flattens a 2D array\n",
    "            2. creates rule for transform on the flattened array.\n",
    "            \"\"\"\n",
    "            \n",
    "            super().fit(X.reshape(-1, 1), y)\n",
    "            \n",
    "            return self\n",
    "   \n",
    "        def transform(self, X):\n",
    "            \"\"\"\n",
    "            1. flattens a 2D array\n",
    "            2. transforms it\n",
    "            3. restores it ot its original shape\n",
    "            \"\"\"\n",
    "            \n",
    "            X_t = super().transform(X.reshape(-1, 1))\n",
    "            return X_t.reshape(-1, X.shape[1])\n",
    "        \n",
    "        def fit_transform(self, X, y = None):\n",
    "            \"\"\"\n",
    "            Fits and transforms 2D array via flattening.\n",
    "            \"\"\"\n",
    "            \n",
    "            x_t = super().fit_transform(X.reshape(-1, 1), y)\n",
    "            return x_t.reshape(-1, X.shape[1])\n",
    "\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Power transform on split data\n",
    "\n",
    "We should remember ourselves that the temperature and precipitation data have to be scaled separately.\n",
    "\n",
    "For this end, using one transformer instance avoiding splitting the data, we write a class which applies transormers to subsets of features. It is akin to [`FeatureUnion`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html). The main only difference is, that a certain transform is applied to a specified section of the dimensions as opposed to the entire data. This is implemented as the `SegmentedFeatureUnion` class below. We are going to subclass the aforementioned `FeatureUnion` class, so that its methods can safely be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion, _fit_one_transformer, _transform_one\n",
    "from sklearn.utils._joblib import delayed, Parallel\n",
    "\n",
    "class SegmentedFeatureUnion(FeatureUnion):\n",
    "    \"\"\"\n",
    "    Applies transformers to selections of features.\n",
    "    Class based on sklearn.pipeline FeatureUnion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, transformer_list, n_jobs = None,\n",
    "                 transformer_weights = None, transformer_ranges = None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            transformer_list ([object]) : list of objects that implements the methods \n",
    "                fit : fit(X[, y])\n",
    "                transform : transform(X, [,y])\n",
    "            transformer_ranges ([np.ndarray]) : list of indices to which the transformers are applied. \n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__(transformer_list, \n",
    "                         n_jobs = n_jobs, \n",
    "                         transformer_weights = transformer_weights)\n",
    "        \n",
    "        self.transformer_ranges = transformer_ranges\n",
    "\n",
    "    def _iter(self):\n",
    "        \"\"\"\n",
    "        Generate (name, trans, weight, transformer_range) tuples excluding None and\n",
    "        'drop' transformers.\n",
    "        \"\"\"\n",
    "        get_weight = (self.transformer_weights or {}).get\n",
    "        \n",
    "        return ((name, trans, get_weight(name), transformer_range)\n",
    "                    for (name, trans), transformer_range in zip(self.transformer_list, self.transformer_ranges)\n",
    "                        if trans is not None and trans != 'drop')\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        \"\"\"\n",
    "        Fit all transformers using X.\n",
    "        \n",
    "        Parameters:\n",
    "            X (np.ndarray[n_samples, n_features]) : data to be transformed\n",
    "            y ({np.ndarray[n_samples], None}) : target variable, usually not used\n",
    "            \n",
    "        Returns:\n",
    "            self (SegmentedFeatureUnion) : this estimator\n",
    "        \"\"\"\n",
    "        \n",
    "        self.transformer_list = list(self.transformer_list)\n",
    "        self._validate_transformers()\n",
    "        transformers = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_fit_one_transformer)(trans, X[:, transform_range], y)\n",
    "                for _, trans, _, transform_range in self._iter())\n",
    "        \n",
    "        # save fitted transformers --> used in self._iter\n",
    "        self._update_transformer_list(transformers)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y = None):\n",
    "        \"\"\"\n",
    "        Transform X separately by each transformer, concatenate results.\n",
    "        Parameters\n",
    "            X (np.ndarray[n_samples, n_features]) : data to be transformed\n",
    "            y ({np.ndarray[n_samples], None}) : ignored\n",
    "            \n",
    "        Returns\n",
    "            X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)\n",
    "            hstack of results of transformers. sum_n_components is the\n",
    "            sum of n_components (output dimension) over transformers.\n",
    "        \"\"\"\n",
    "        \n",
    "        x_t = Parallel(n_jobs=self.n_jobs)(\n",
    "            delayed(_transform_one)(trans, X[:, transform_range], None, weight)\n",
    "                for name, trans, weight, transform_range in self._iter())\n",
    "        \n",
    "        # stack arrays\n",
    "        X_t = np.hstack(X_t)\n",
    "        \n",
    "        return X_t\n",
    "    \n",
    "    def fit_transform(self, X, y = None):\n",
    "        \"\"\"\n",
    "        Override parents fittransform.\n",
    "        \"\"\"\n",
    "        return self.fit(X, y).transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "\n",
    "We can now transform the data according to the following recipe\n",
    "* split features to temperature and precipitation subspaces\n",
    "* flatten each subspace\n",
    "* power transform each flattened arrays\n",
    "* mould array to original shape\n",
    "\n",
    "The power transform will be performed by the `FlatPowerTransformer` class instances. The underlying class is the `PowerTransformer` utility of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "FlatPowerTransformer = create_flat_transformer(PowerTransformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate transformers are then created for the temperature and precipitations subspaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers = [(\"trf1\", FlatPowerTransformer(method = 'yeo-johnson', standardize = True)),\n",
    "                (\"trf2\", FlatPowerTransformer(method = 'yeo-johnson', standardize = True))]\n",
    "\n",
    "# 0-->11 temperature; 12-->23 precipitation\n",
    "transformer_ranges = [np.arange(12), np.arange(12, 24)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformers are then collated in a single `SegmentedFeatureUnion` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfu = SegmentedFeatureUnion(transformers, transformer_ranges = transformer_ranges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test is worth the time on a random array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Are the two methods agree within error?\n",
      "-The computer says: Yes!\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(100, 24)\n",
    "\n",
    "# segmented feature union\n",
    "sfu.fit(X)\n",
    "X_t = sfu.transform(X)\n",
    "\n",
    "# setp-by-step\n",
    "trf1 = PowerTransformer(method = 'yeo-johnson', standardize = True)\n",
    "trf2 = PowerTransformer(method = 'yeo-johnson', standardize = True)\n",
    "\n",
    "trf1.fit(X[:,np.arange(12)].reshape(-1, 1))\n",
    "trf2.fit(X[:,np.arange(12,24)].reshape(-1, 1))\n",
    "\n",
    "X_t_ref = np.hstack([trf1.transform(X[:,np.arange(12)].reshape(-1, 1)).reshape(-1,12),\n",
    "                     trf2.transform(X[:,np.arange(12,24)].reshape(-1, 1)).reshape(-1, 12)])\n",
    "\n",
    "# compare two transformed arrays\n",
    "print(\"-Are the two methods agree within error?\")\n",
    "print(\"-The computer says: {0}\".format(('No-oo...', 'Yes!')[np.allclose(X_t, X_t_ref)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "It is healthy if the input values are in the range of $[0,1]$. This can be achieved by appending the sklearn's very own `MinmaxScaler` after the Yeo--Johnson transform. Since both the temperature and precipitation data are now scaled roughly to the same range, it is reasonable to apply the `MinMaxScaler` to the all dimensions of the numeric data without splitting them. The transform should digest the flattened array, thus the we will reuse our `create_flat_transformer` class factory to create the appropriate transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# create a minmax scaler operating on flattened arrays\n",
    "FlatMinMaxScaler = create_flat_transformer(MinMaxScaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "As promised in the introduction a neural network will be in charge of estalishing a relationship between numerical data and the climate class labels. The network of choice is `tensorflow`'s `keras` high level framework. This module contains a wrapper that creates an estimator class compatible with the `sklearn` API. \n",
    "\n",
    "`KerasClassifier` constructor consumes keyword argument. The `build_fn` keyword requires a function that builds and compiles a network. Additional, user defined keywords can also be passed that are either used to specify the network properties or define particulars of the `fit` process, such as number of epochs or batch size.\n",
    "\n",
    "Below a slim manifestation of such a function, `model_builder` is defined. The sole parameter of this function is the keyword argument _layers_:\n",
    "\n",
    "* `layers` : defines the size and activation of the successive layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hornu\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:72: UserWarning: h5py is running against HDF5 1.10.2 when it was built against 1.10.3, this may cause problems\n",
      "  '{0}.{1}.{2}'.format(*version.hdf5_built_version_tuple)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(layers = [[10, 'relu'], [10, 'softmax']]):\n",
    "    \"\"\"\n",
    "    Mock neural network buider.\n",
    "    Parameters:\n",
    "        layers [[int, str]] : list of lists, where\n",
    "            1. member of tuple : layer size\n",
    "            2. member of tuple : activation type. \n",
    "            Default : [(10, 'relu'), (10, 'softmax')]\n",
    "    Returns\n",
    "        model () : a compiled neural network\n",
    "    \"\"\"\n",
    "    \n",
    "    # argument checks -- let us assume types are correct\n",
    "    allowed_activations = ('relu', 'softmax')\n",
    "    \n",
    "    for idx, (nunit, activation) in enumerate(layers):\n",
    "         \n",
    "        if not activation in allowed_activations:\n",
    "            raise ValueError(\"'activation' must be {0} in position ({1}, 1). Got: {2}\" \\\n",
    "                             .format(\" \".join(allowed_activations), idx, activation))\n",
    "\n",
    "        if nunit < 1:\n",
    "            raise ValueError(\"'nunit' must be greater than zero in position({0}, 1). Got: {1}\" \\\n",
    "                             .format(idx, nunit))\n",
    "            \n",
    "            # build model\n",
    "    model = keras.models.Sequential(\n",
    "                [keras.layers.Dense(nunit, activation = activation) \n",
    "                    for nunit, activation in layers])\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So good, so far. We do not the number of labels in advance wich determines the size of the output layer. In fact, it is unknown until the training happens. The problem is the `layer` keyword argument is passed to the constructor and saved. There is no way updating it when the fit happens in the `KerasClassifier` class. There is a solution, though. The actual model that performs the classification is not constructed until the `fit` method is called. By modifying this method in a subclass of `KerasClassifier` one can set the size of the last layer dynamically.\n",
    "\n",
    "An issue immediately arises. The parameters of the `model_builder` cannot be accessed without reflection inside the `KerasClassifier` class. It is therefore not possible to use `set_params` to update the layers, unless the keyword `layers` is explicitily passed to the constructor thereof. Of course, it is legit to always pass this argument, but it is unwiedly and gives rise to making plenty of errors.\n",
    "\n",
    "Using the `inspect` module is modestly elegant, but the most straighforward solution. In fact, `KerasClassifier` realies on it to determine whether a keyword argument is meant to modify the network structure, or it is a directive to the `compile` or `fit` methods.\n",
    "\n",
    "Below, the `AdaptiveKerasClassifier` class modifies the original `fit` algorithm, so that the size of the last layer is automatically set by the number of classes in the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "class AdaptiveKerasClassifier(KerasClassifier):\n",
    "    \n",
    "    def fit(self, x, y, **kwargs):\n",
    "        \"\"\"\n",
    "        Trains the neural network on the a given sample.\n",
    "        This method uses KerasClassifier's fit method, but updates the size of the last layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        n_unit_output_layer = np.unique(y).size\n",
    "        \n",
    "        # update the size of the last layer\n",
    "\n",
    "        # 'layers' has been passed explicitly as a parameter\n",
    "        if 'layers' in self.sk_params: \n",
    "            \n",
    "            # copy and modify\n",
    "            layers_ = [[n_, a_] for n_, a_ in self.sk_params['layers']]\n",
    "            layers_[-1][0] = n_unit_output_layer\n",
    "            layers_ = tuple(layers_)\n",
    "\n",
    "            # reset sk_params\n",
    "            self.sk_params['layers'] = layers_\n",
    "            \n",
    "        # if not in skparams, build_fn uses the default value\n",
    "        else:\n",
    "            # get default with introspection\n",
    "            layers_ = inspect.signature(self.build_fn).parameters['layers'].default\n",
    "\n",
    "            # modify it\n",
    "            layers_[-1][0] = n_unit_output_layer\n",
    "            \n",
    "            # add to sk_params\n",
    "            self.sk_params['layers'] = layers_\n",
    "         \n",
    "        # invoke parent's fit method\n",
    "        super().fit(x, y, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assempling the pipeline\n",
    "\n",
    "### Loading data\n",
    "\n",
    "A simple utility is written to load the raw data and return the numeric observations and labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path_to_db):\n",
    "    \"\"\"\n",
    "    Loads the climate data.\n",
    "    Parameters: \n",
    "        path_to_db (str) : path to the csv dat file\n",
    "    Returns:\n",
    "        X (np.ndarray[n_samples, n_features], dtype = float) : the observations\n",
    "        labels (np.ndarray[n_samples], dtype = object) : the climate class labels\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(path_to_db)\n",
    "    \n",
    "    X = df.loc[:,'2' : '25'].values\n",
    "    \n",
    "    labels = df['Cls'].values\n",
    "    \n",
    "    # free up space\n",
    "    del df\n",
    "    \n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are then loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_db = r'C:\\Users\\hornu\\OneDrive\\Documents\\personal\\balazs-personal\\balazs-personal\\repos\\Weather\\data\\climate\\temperature-precipitation-cls.csv'\n",
    "X, labels = data_loader(path_to_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels\n",
    "\n",
    "If the classes are unbalanced it can happen that the training and test sets of labels are not identical. As a consequence, one has take extra care to ensure that they are encoded consistently. To save ourselves from this trouble, the literal labels are encoded for the entire data set before model optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pipeline\n",
    "\n",
    "We now have all the components of the pipeine to classify the climate data set. These are\n",
    "\n",
    "1. Segmented feature transformation using our `SegmentedFeatureUnion` class\n",
    "2. Scaling of the entire data set with the `FlatMinMaxScaler`\n",
    "3. Neural network classification algorithm wrapped in the `AdaptiveKerasClassifier` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# power transform\n",
    "power_transformers = [(\"power_trf1\", FlatPowerTransformer(method = 'yeo-johnson', standardize = True)),\n",
    "                      (\"power_trf2\", FlatPowerTransformer(method = 'yeo-johnson', standardize = True))]\n",
    "\n",
    "power_trf = SegmentedFeatureUnion(power_transformers, \n",
    "                                  transformer_ranges = [np.arange(12), \n",
    "                                                        np.arange(12, 24)])\n",
    "\n",
    "# scaling\n",
    "scale_trf = FlatMinMaxScaler()\n",
    "\n",
    "# classifier\n",
    "classifier = AdaptiveKerasClassifier(model_builder)\n",
    "\n",
    "# full pipeline\n",
    "pipeline1 = Pipeline([('power_trf', power_trf), \n",
    "                     ('scale_trf', scale_trf), \n",
    "                     ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation\n",
    "\n",
    "Set of parameters we seek to optimise are\n",
    "* compositon of preprocessing pipeline\n",
    "* network architecture\n",
    "* number of epochs\n",
    "\n",
    "The optimisation is carried out on a grid where the fitted models are evaluated by a fivefold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "\n",
    "# test various network architectures.\n",
    "parameter_grid = [{'classifier__epochs' : [5, 7, 10], \n",
    "                   'classifier__layers' : [\n",
    "                               ((128, 'relu'), (10, 'softmax')), \n",
    "                               ((24, 'relu'), (128, 'relu'), (10, 'softmax'))] \n",
    "                  }]\n",
    "\n",
    "# five fold crossvalidation\n",
    "cv = KFold(n_splits = 5, shuffle = True)\n",
    "\n",
    "# using pipeline with minmax scaling\n",
    "grid1 = GridSearchCV(pipeline1, \n",
    "                    param_grid = parameter_grid, \n",
    "                    cv = cv, \n",
    "                    return_train_score = False)\n",
    "\n",
    "grid1.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to investigate the effect of the MinMaxScaler by omitting it from the pipeline. This requires updating our pipeline and cross validation object which can be done in two lines. Fortunately, we save an immense amount of work using pipeline and custom esitmators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline without minmax scaling\n",
    "pipeline2 = Pipeline([('power_trf', power_trf),  \n",
    "                     ('classifier', classifier)])\n",
    "\n",
    "grid2 = GridSearchCV(pipeline2, \n",
    "                    param_grid = parameter_grid, \n",
    "                    cv = cv, \n",
    "                    return_train_score = False)\n",
    "\n",
    "grid2.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline with minmax scaling returns results less good (I listened too much PM questions, thus I avoid using the term 'worse') than the one from which the scaler is omitted. The reason being bulk of the data is squashed in a region close to zero, which results in low activations in the first relu layer. In other words, we lose information.\n",
    "\n",
    "The network with two hidden layers outperforms the one with a single middle layer. This is not at all surprising, for it has more flexibility and thus compensate for the information loss.\n",
    "\n",
    "There is not significant difference between the two networks where the minmax scaler is not used. One hidden layer has enough variable to parametrise a reasonably good classifier, for the input data are not squashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO HIDE\n",
    "\n",
    "res1 = pd.DataFrame(grid1.cv_results_)\n",
    "res2 = pd.DataFrame(grid2.cv_results_)\n",
    "\n",
    "# encode layer types\n",
    "le = LabelEncoder()\n",
    "res1['layer_type'] = le.fit_transform(res1['param_classifier__layers'].apply(lambda x: str(x)))\n",
    "res2['layer_type'] = le.fit_transform(res2['param_classifier__layers'].apply(lambda x: str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TO HIDE\n",
    "\n",
    "def plot_one_set(data, ax, color, label, title):\n",
    "    \"\"\"\n",
    "    plots one set of results.\n",
    "    \"\"\"\n",
    "    ax.errorbar(data['param_classifier__epochs'], \n",
    "                data['mean_test_score'], \n",
    "                yerr = data['std_test_score'],\n",
    "                label = label,\n",
    "                fmt = 'o', color = color, \n",
    "                capthick = 2, capsize = 2)\n",
    "\n",
    "    ax.legend(loc = 3)\n",
    "    ax.set_ylim((0.7, 0.9))\n",
    "    ax.grid(True); ax.set_xlabel(\"number of epochs\"); ax.set_ylabel(\"accuracy\")\n",
    "    ax.set_title(title)\n",
    "    \n",
    "# no, I won't use seaborn\n",
    "fig, axes = plt.subplots(1, 2, sharey = True)\n",
    "fig.set_size_inches(8, 4)\n",
    "\n",
    "# with minmax\n",
    "plot_one_set(res1.loc[res1['layer_type'] == 0], axes[0], 'orange', 'one layer', 'MinMax Scaler')\n",
    "plot_one_set(res1.loc[res1['layer_type'] == 1], axes[0], 'red', 'two layers', 'MinMax Scaler')\n",
    "\n",
    "# without minmax\n",
    "plot_one_set(res2.loc[res2['layer_type'] == 0], axes[1], 'orange', 'one layer', 'No Minmax Scaler')\n",
    "plot_one_set(res2.loc[res2['layer_type'] == 1], axes[1], 'red', 'two layers', 'No MinMax Scaler')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have created a data pipeline that transforms and classifies the climate data set. Most of the work consisted of writing wrappers around existing `sklearn` and `tensorflow` utilities.\n",
    "\n",
    "Neaural network classifiers have also been trained and their performance examined. It has been established the range of the input data has a significant impact on the accuracy of the the classifiers.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
